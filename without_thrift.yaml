version: '3.8'

x-airflow-common: &airflow-common
  build: 
    context: ./airflow
    dockerfile: Dockerfile
  environment:
    - AIRFLOW__CORE__EXECUTOR=LocalExecutor
    - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow
    - AIRFLOW__CORE__FERNET_KEY=FB0o_zt4e3Ziq4jTzPNJXLiVr3AgK7JQUEzY_LgG0A4=
    - AIRFLOW__CORE__LOAD_EXAMPLES=false
    - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
    - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=true
    - AIRFLOW_CONN_SPORT_ADVANTAGES_DB=postgresql://${SPORT_POSTGRES_USER}:${SPORT_POSTGRES_PASSWORD}@business-postgres:${SPORT_POSTGRES_INTERNAL_PORT}/${SPORT_POSTGRES_DB}
    - AIRFLOW_CONN_REDPANDA=kafka://redpanda:9092
    - AIRFLOW_CONN_SPARK_DEFAULT=spark://spark-master:7077
    - MINIO_ROOT_USER=${MINIO_ROOT_USER}
    - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD}
    - DB_HOST=${SPORT_POSTGRES_HOST}
    - DB_USER=${SPORT_POSTGRES_USER}
    - DB_PASSWORD=${SPORT_POSTGRES_PASSWORD}
    - DB_NAME=${SPORT_POSTGRES_DB}
    - DB_PORT=${SPORT_POSTGRES_INTERNAL_PORT}
    - GUNICORN_TIMEOUT=300
  volumes:
    - ./airflow/dags:/opt/airflow/dags
    - ./airflow/logs:/opt/airflow/logs
    - ./airflow/plugins:/opt/airflow/plugins
    - ./airflow/config:/opt/airflow/config
  depends_on:
    - airflow-postgres
    - spark-master
    - minio
  networks:
    - sport_network

services:
 # BDD Postgresql pour Airflow
  airflow-postgres:
    image: postgres:14
    container_name: sport-advantages-airflow-postgres
    restart: always
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    volumes:
      - airflow_postgres_data:/var/lib/postgresql/data
    ports:
      - "${AIRFLOW_POSTGRES_PORT}:5432"
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      retries: 5
    networks:
      - sport_network

  # Base de données PostgreSQL
  business-postgres:
    image: postgres:14
    container_name: business-postgres
    restart: always
    environment:
      POSTGRES_USER: ${SPORT_POSTGRES_USER}
      POSTGRES_PASSWORD: ${SPORT_POSTGRES_PASSWORD}
      POSTGRES_DB: ${SPORT_POSTGRES_DB}
    ports:
      - "${SPORT_POSTGRES_PORT}:${SPORT_POSTGRES_INTERNAL_PORT}"
    volumes:
      - business_postgres_data:/var/lib/postgresql/data
      - ./init-scripts:/docker-entrypoint-initdb.d
      - ./postgres-config/postgresql.conf:/etc/postgresql/postgresql.conf
    command: >
      postgres
      -c config_file=/etc/postgresql/postgresql.conf
      -c shared_preload_libraries=pgoutput
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${SPORT_POSTGRES_USER} -d ${SPORT_POSTGRES_DB}"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - sport_network

  # Redpanda - Alternative à Kafka pour le streaming de données
  redpanda:
    image: redpandadata/redpanda:latest
    container_name: sport-advantages-redpanda
    command:
    - redpanda
    - start
      --smp=1
      --memory=1G
      --reserve-memory=0M
      --overprovisioned
      --node-id=0
      --check=false
      --kafka-addr PLAINTEXT://0.0.0.0:9092,OUTSIDE://0.0.0.0:19092
      --advertise-kafka-addr PLAINTEXT://redpanda:9092,OUTSIDE://localhost:19092
      --rpc-addr redpanda:33145
      --advertise-rpc-addr redpanda:33145
      --set redpanda.enable_transactions=true
      --set redpanda.enable_idempotence=true
    ports:
      - "9092:9092"
      - "19092:19092"
      - "33145:33145"
    volumes:
      - redpanda_data:/var/lib/redpanda/data
    depends_on:
      business-postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "rpk cluster health | grep -E 'Healthy:.+true' || exit 1"]
      interval: 15s
      timeout: 3s
      retries: 5
      start_period: 5s
    networks:
      - sport_network

  # Redpanda Console - Interface UI pour Redpanda
  redpanda-console:
    image: redpandadata/console:latest
    container_name: sport-advantages-redpanda-console
    depends_on:
      redpanda:
        condition: service_healthy
    ports:
      - "8080:8080"
    environment:
      - KAFKA_BROKERS=redpanda:9092
    networks:
      - sport_network

  # Debezium - Pour capturer les changements de la base de données (CDC)
  debezium:
    image: debezium/connect:2.4
    container_name: sport-advantages-debezium
    depends_on:
      business-postgres:
        condition: service_healthy
      redpanda:
        condition: service_healthy
    ports:
      - "8083:8083"
    environment:
      - CONNECT_REST_ADVERTISED_HOST_NAME=debezium
      - GROUP_ID=${DEBEZIUM_GROUP_ID}
      - BOOTSTRAP_SERVERS=redpanda:9092
      - CONFIG_STORAGE_TOPIC=${DEBEZIUM_CONFIG_STORAGE_TOPIC}
      - OFFSET_STORAGE_TOPIC=${DEBEZIUM_OFFSET_STORAGE_TOPIC}
      - STATUS_STORAGE_TOPIC=${DEBEZIUM_STATUS_STORAGE_TOPIC}
      - CONNECT_KEY_CONVERTER=org.apache.kafka.connect.json.JsonConverter
      - CONNECT_VALUE_CONVERTER=org.apache.kafka.connect.json.JsonConverter
    healthcheck: 
      test: ["CMD-SHELL", "curl -f http://debezium:8083/connectors"]
      interval: 10s
      timeout: 5s
      retries: 15
      start_period: 30s
    command: >
      sh -c "
        /docker-entrypoint.sh start &
        for i in {1..10}; do
          if curl -s -f http://localhost:8083/; then
            curl -X POST http://localhost:8083/connectors -H 'Content-Type: application/json' -d '{
              \"name\": \"sport-advantages-postgres-connector\",
              \"config\": {
                \"connector.class\": \"io.debezium.connector.postgresql.PostgresConnector\",
                \"database.hostname\": \"business-postgres\",
                \"database.port\": \"5432\",
                \"database.user\": \"${SPORT_POSTGRES_USER}\",
                \"database.password\": \"${SPORT_POSTGRES_PASSWORD}\",
                \"database.dbname\": \"${SPORT_POSTGRES_DB}\",
                \"database.server.name\": \"sport-advantages\",
                \"table.include.list\": \"sport_advantages.sport_activities\",
                \"plugin.name\": \"pgoutput\",
                \"topic.prefix\": \"sport\",
                \"snapshot.mode\": \"initial\"
              }
            }' && break;
          fi
          sleep $((2 * i)) # Backoff exponentiel (2, 4, 6,... secondes)
        done
        tail -f /dev/null"
    networks:
      - sport_network

  # Pour valider les données de transport et importation de données salariés 
  init-db-commute-validation:
    build:
      context: ./commute_validation
      dockerfile: Dockerfile
    container_name: sport-advantages-init-db
    depends_on:
      business-postgres:
        condition: service_healthy
    volumes:
      - ./commute_validation:/app
    environment:
      - DB_HOST=${SPORT_POSTGRES_HOST}
      - DB_USER=${SPORT_POSTGRES_USER}
      - DB_PASSWORD=${SPORT_POSTGRES_PASSWORD}
      - DB_NAME=${SPORT_POSTGRES_DB}
      - DB_PORT=${SPORT_POSTGRES_INTERNAL_PORT}
      - GOOGLE_MAPS_API_KEY=${GOOGLE_MAPS_API_KEY}
    networks:
      - sport_network

  # Service Python pour générer les données et les insérer dans PostgreSQL
  data-generator:
    build:
      context: ./activity_generator
      dockerfile: Dockerfile
    container_name: sport-advantages-data-generator
    depends_on:
      init-db-commute-validation:
        condition: service_completed_successfully
      debezium:
        condition: service_healthy
    volumes:
      - ./activity_generator:/app
    environment:
      - DB_HOST=${SPORT_POSTGRES_HOST}
      - DB_USER=${SPORT_POSTGRES_USER}
      - DB_PASSWORD=${SPORT_POSTGRES_PASSWORD}
      - DB_PORT=${SPORT_POSTGRES_INTERNAL_PORT}
      - DB_NAME=${SPORT_POSTGRES_DB}
    networks:
      - sport_network
  
  # Spark Master
  spark-master:
    image: bitnami/spark:3.2.3
    container_name: sport-advantages-spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_WEBUI_PORT=8080
    volumes:
      - spark_master_data:/recovery
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 10s
      timeout: 5s
      retries: 5
    mem_limit: 1g
    cpus: 1.0
    ports:
      - "8090:8080"
      - "7077:7077"
    networks:
      - sport_network
   
  # Spark Worker
  spark-worker:
    image: bitnami/spark:3.2.3
    container_name: sport-advantages-spark-worker
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2g
      - SPARK_WORKER_CORES=2
      - SPARK_EXECUTOR_HEARTBEAT_INTERVAL=10s
      - SPARK_NETWORK_TIMEOUT=800s
      - SPARK_TASK_MAXFAILURES=4                        
      - SPARK_SPECULATION=true 
      - SPARK_RPC_AUTHENTICATION_ENABLED=no       
      - SPARK_RPC_ENCRYPTION_ENABLED=no           
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no 
      - SPARK_SSL_ENABLED=no 
    mem_limit: 2g
    cpus: 2.0
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - sport_network

  scala-runner:
    build:
      context: ./spark
      dockerfile: Dockerfile
    container_name: sport-advantages-scala-runner
    environment:
      - KAFKA_SERVERS=redpanda:9092
      # - TOPIC_NAME=${DEBEZIUM_OFFSET_STORAGE_TOPIC}
      - TOPIC_NAME=sport.sport_advantages.sport_activities
      - DB_HOST=${SPORT_POSTGRES_HOST}
      - DB_USER=${SPORT_POSTGRES_USER}
      - DB_PASSWORD=${SPORT_POSTGRES_PASSWORD}
      - DB_NAME=${SPORT_POSTGRES_DB}
      - DB_PORT=${SPORT_POSTGRES_INTERNAL_PORT}
      - MINIO_ENDPOINT=minio:9000
      - MINIO_ROOT_USER=${MINIO_ROOT_USER}
      - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD}
      - AWS_ACCESS_KEY_ID=${MINIO_ROOT_USER}
      - AWS_SECRET_ACCESS_KEY=${MINIO_ROOT_PASSWORD}
    depends_on:
      minio:
        condition: service_healthy
      debezium:
        condition: service_healthy
    volumes:
      - ./spark:/scripts
      - delta_volume:/data
    networks:
      - sport_network

  # Minio comme stockage compatible S3 pour Delta Lake
  minio:
    image: minio/minio
    container_name: sport-advantages-minio
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      - MINIO_ROOT_USER=${MINIO_ROOT_USER}
      - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD}
    command: server /data --console-address ":9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 10s
      timeout: 5s
      retries: 5
    volumes:
      - minio_data:/data
    networks:
      - sport_network

  # Initialisation de MinIO
  minio-init:
    image: minio/mc
    depends_on:
      - minio
    entrypoint: >
      /bin/sh -c "
      until (/usr/bin/mc config host add myminio http://minio:9000 ${MINIO_ROOT_USER} ${MINIO_ROOT_PASSWORD}) do sleep 1; done;
      /usr/bin/mc mb myminio/delta-tables --ignore-existing;
      /usr/bin/mc policy set public myminio/delta-tables;
      exit 0;
      "
    networks:
      - sport_network

  # Airflow Webserver
  airflow-webserver:
    <<: *airflow-common
    container_name: sport-advantages-airflow-webserver
    command: webserver
    restart: always
    depends_on:
      airflow-postgres:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
    ports:
      - "8081:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - sport_network

  # Airflow Scheduler
  airflow-scheduler:
    <<: *airflow-common
    container_name: sport-advantages-airflow-scheduler
    command: scheduler
    depends_on:
      - airflow-postgres
      - airflow-init
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8974/health"]
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - sport_network

  # Airflow Init (initialise la base de données et crée le premier utilisateur)
  airflow-init:
    <<: *airflow-common
    container_name: sport-advantages-airflow-init
    entrypoint: /bin/bash
    command:
      - -c
      - |
        set -e
        if [[ -z "$(ls -A /opt/airflow/logs)" ]]; then
          mkdir -p /opt/airflow/logs
        fi
        airflow db init
        airflow users create \
          --username admin \
          --password admin \
          --firstname Admin \
          --lastname User \
          --role Admin \
          --email admin@example.com
    restart: on-failure
    mem_limit: 2g    
    cpus: 2.0  
    depends_on:
      - airflow-postgres
    networks:
      - sport_network

 # Service PostgreSQL pour les métadonnées de Superset
  superset-postgres:
    image: postgres:14
    container_name: superset-postgres
    environment:
      - POSTGRES_USER=superset
      - POSTGRES_PASSWORD=superset
      - POSTGRES_DB=superset
    volumes:
      - superset_postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      retries: 5
    networks:
      - sport_network

  # Service Spark Thrift Server pour exposer les données via SQL
  spark-thrift:
    build:
      context: ./spark
      dockerfile: Dockerfile.thrift
    container_name: sport-advantages-spark-thrift
    hostname: spark-thrift
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_LOCAL_IP=spark-thrift
      - SPARK_DRIVER_HOST=spark-thrift
      - MINIO_ROOT_USER=${MINIO_ROOT_USER}
      - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD}
      - HADOOP_CONF_DIR=/opt/bitnami/spark/conf
      - SPARK_CLASSPATH=/opt/bitnami/spark/jars/*
    command: >
      bash -c "
      /opt/bitnami/spark/sbin/start-thriftserver.sh \
      --master spark://spark-master:7077 \
      --hiveconf hive.server2.thrift.port=10000 \
      --hiveconf hive.server2.thrift.bind.host=0.0.0.0 \
      --conf spark.hadoop.fs.s3a.access.key=${MINIO_ROOT_USER} \
      --conf spark.hadoop.fs.s3a.secret.key=${MINIO_ROOT_PASSWORD} \
      --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000 \
      --conf spark.hadoop.fs.s3a.path.style.access=true"
    ports:
      - "10000:10000"
    volumes:
    - ./spark/warehouse:/opt/bitnami/spark/warehouse
    - ./spark/jars:/opt/bitnami/spark/jars/
    depends_on:
      - spark-master
      - minio
    networks:
      sport_network:
        aliases:
          - spark-thrift

  superset:
    build:
      context: ./superset
      dockerfile: Dockerfile
    container_name: sport-advantages-superset
    restart: always
    depends_on:
      superset-postgres:
        condition: service_healthy
      - trino
    environment:
      # Explicitly set a strong secret key
      - SUPERSET_SECRET_KEY=ThisIsAVeryLongAndSecureKeyForOurSupersetInstance1234567890
      - SUPERSET_DATABASE_URI=postgresql+psycopg2://superset:superset@superset-postgres:5432/superset
      - POSTGRES_USER=${SPORT_POSTGRES_USER}
      - POSTGRES_PASSWORD=${SPORT_POSTGRES_PASSWORD}
      - POSTGRES_DB=${SPORT_POSTGRES_DB}
      - POSTGRES_HOST=business-postgres
      - POSTGRES_PORT=${SPORT_POSTGRES_INTERNAL_PORT}
      - MINIO_ROOT_USER=${MINIO_ROOT_USER}
      - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD}
      - PYTHONPATH=/app
      - FLASK_APP=superset.app:create_app()
      - FLASK_ENV=production
      # Set this to ignore security key warnings in dev environment
      - SUPERSET_ENV=development
      - TRINO_HOST=trino
      - TRINO_PORT=8080
      - TRINO_USER=trino
      - TRINO_CATALOG=postgresql,minio
    ports:
      - "8088:8088"
    volumes:
      - ./superset/superset_data:/app/superset_home
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8088/health"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - sport_network

volumes:
  business_postgres_data:
  airflow_postgres_data:
  minio_data:
  redpanda_data:
  spark_master_data:
  delta_volume:
  superset_postgres_data:

networks:
  sport_network:
    driver: bridge
    name: sport_network